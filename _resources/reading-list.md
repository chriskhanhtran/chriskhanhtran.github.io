---
title: "Reading List"
toc: true
toc_label: "Table of Contents"
toc_icon: "bookmark"
excerpt: "My reading list of research papers, articles, books and authors in AI."
header:
  teaser: "https://images.unsplash.com/photo-1544716278-e513176f20b5?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1267&q=80"

---
My reading list of research papers, articles, books and authors in Deep Learning, Natural Language Processing and Computer Vision.

**Key:**
- ✅ = Have read
- 👨‍🔬 = Have implemented
- ⏳ = Reading / Want to read

# 1. Research Papers and Articles
## 1.1. Natural Language Processing
### Sequence-to-sequence
- ⏳ [PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization](https://ai.googleblog.com/2020/06/pegasus-state-of-art-model-for.html)
- 👨‍🔬 [PreSumm: Text Summarization with Pretrained Encoders](https://github.com/nlpyang/PreSumm)
- ✅ [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/pdf/1910.13461.pdf)
- ✅ [T5: Text-To-Text Transfer Transformer](https://github.com/google-research/text-to-text-transfer-transformer)

### Transformers and Pretrained Language Models
- ⏳ [Longformer: The Long-Document Transformer](https://github.com/allenai/longformer)
- ⏳ [Movement Pruning: Adaptive Sparsity by Fine-Tuning](https://arxiv.org/abs/2005.07683)
- 👨‍🔬 [MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices](https://arxiv.org/abs/2004.02984)
- 👨‍🔬 [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://medium.com/huggingface/distilbert-8cf3380435b5)
- 👨‍🔬 [ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators](https://github.com/google-research/electra)
- 👨‍🔬 [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692)
- 👨‍🔬 [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
- ✅ [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)
- ✅ [Attention is All You Need](https://arxiv.org/abs/1706.03762)

### Sequence Classification
- 👨‍🔬 [A Sensitivity Analysis of (and Practitioners' Guide to) Convolutional Neural Networks for Sentence Classification](https://arxiv.org/abs/1510.03820)
- 👨‍🔬 [Convolutional Neural Networks for Sentence Classification](https://arxiv.org/abs/1408.5882)
- ✅ Baselines and Bigrams

### Word Embeddings
- ✅ [Concatenated Power Mean Word Embeddings as Universal Cross-Lingual Sentence Representations](https://arxiv.org/abs/1803.01400)
- ✅ [Efficient Sentence Embedding using Discrete Cosine Transform](https://arxiv.org/abs/1909.03104)
- ✅ [FastText: Advances in Pre-Training Distributed Word Representations](https://fasttext.cc/docs/en/english-vectors.html)
- ✅ [Word2vec: Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1310.4546.pdf)

## 1.2. Computer Vision
### Object Detection and Semantic Segmentation
- ⏳ [Detectron2](https://github.com/facebookresearch/detectron2)
- ⏳ [U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597)
- ⏳ [Mask R-CNN](https://arxiv.org/abs/1703.06870)
- ⏳ [Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks](https://arxiv.org/abs/1506.01497)
- ⏳ [Fast R-CNN](https://arxiv.org/abs/1504.08083)

### Image Enhancing
- ✅ [ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks](https://arxiv.org/abs/1809.00219)
- ✅ [Deep White-Balance Editing](https://arxiv.org/abs/2004.01354)

### Style Transfer
- ⏳ [Perceptual Losses for Real-Time Style Transfer and Super-Resolution](https://arxiv.org/abs/1603.08155)
- 👨‍🔬 [Image Style Transfer Using Convolutional Neural Networks](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf)

## 1.3. Others
- ⏳ [Graph Neural Networks: A Review of Methods and Applications](https://arxiv.org/pdf/1812.08434.pdf)
- ⏳ [Semi-Supervised Classification with Graph Convolutional Networks](https://arxiv.org/abs/1609.02907)
- ✅ [Deep Reinforcement Learning: Pong from Pixels](http://karpathy.github.io/2016/05/31/rl/)
- 👨‍🔬 [Generative_Adversarial_Networks](https://github.com/chriskhanhtran/CS231n-CV/blob/master/assignment3/Generative_Adversarial_Networks_PyTorch.ipynb)

# 2. Books
- ⏳ [Deep Learning](http://www.deeplearningbook.org/)
- ⏳ [The Elements of Statistical Learning](https://web.stanford.edu/~hastie/Papers/ESLII.pdf)
- ⏳ [The Hundred-Page Machine Learning Book](http://themlbook.com/)

# 3. Blogs
- [Andrej Karpathy](http://karpathy.github.io/)
- [Sebastian Ruder](https://ruder.io/)
- [Chris McCormick](https://mccormickml.com/)
- [Jay Alammar](http://jalammar.github.io/)
